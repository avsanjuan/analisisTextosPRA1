{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.877 · Análisis de sentimientos y textos</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario en Ciencias de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicaciones</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PAC 1: Procesamiento y análisis de información textual\n",
    "\n",
    "En esta práctica revisaremos y aplicaremos los conocimientos aprendidos en el módulo 1. Concretamente trataremos 3 temas.\n",
    "\n",
    "<ul>\n",
    "<li>1. Obtención de datos a partir de información textual\n",
    "<li>2. Detección de temas\n",
    "<li>3. Clasificación de textos\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El propósito de la práctica es descubrir rasgos característicos de las noticias falsas sobre el Covid-19 usando las herramientas explicadas en el módulo 1. Además veremos si es posible clasificar automáticamente noticias falsas con métodos de machine learning. Utilizaremos el dataset <i>corona_fake.csv</i>. Este dataset contiene noticias en inglés sobre el covid-19 etiquetadas según si son noticias falsas (<i>fake</i>) o no. El dataset se organiza en cuatro columnas:\n",
    "\n",
    "<b>title</b>: títular de la noticia<br>\n",
    "<b>text</b>: cuerpo de la noticia<br>\n",
    "<b>source</b>: fuente de la noticia<br>\n",
    "<b>label</b>: etiqueta <i>Fake</i> si la noticia es falsa. Etiqueta <i>TRUE</i> si es verdadera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('all')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  You just need to add water, and the drugs and ...   \n",
       "1  Hydroxychloroquine has been shown to have a 10...   \n",
       "2  Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3  The Corona virus is a man made virus created i...   \n",
       "4  Doesn’t @BillGates finance research at the Wuh...   \n",
       "\n",
       "                      source label  \n",
       "0  coronavirusmedicalkit.com  Fake  \n",
       "1               RudyGiuliani  Fake  \n",
       "2                CharlieKirk  Fake  \n",
       "3    JoanneWrightForCongress  Fake  \n",
       "4    JoanneWrightForCongress  Fake  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"corona_fake.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Obtención de datos a partir de información textual (5 puntos)\n",
    "\n",
    "Primero, cargamos las librerías necesarias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos dos dataframes. El primero (df_fake) contendrá las noticias clasificadas como <i>Fake</i> y el segundo dataframe (df_true) contendrá las noticias clasificadas como <i>TRUE</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Due to the recent outbreak for the Coronavirus...</td>\n",
       "      <td>You just need to add water, and the drugs and ...</td>\n",
       "      <td>coronavirusmedicalkit.com</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Hydroxychloroquine has been shown to have a 10...</td>\n",
       "      <td>RudyGiuliani</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Fact: Hydroxychloroquine has been shown to hav...</td>\n",
       "      <td>CharlieKirk</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>The Corona virus is a man made virus created i...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doesn’t @BillGates finance research at the Wuh...</td>\n",
       "      <td>JoanneWrightForCongress</td>\n",
       "      <td>Fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Due to the recent outbreak for the Coronavirus...   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  You just need to add water, and the drugs and ...   \n",
       "1  Hydroxychloroquine has been shown to have a 10...   \n",
       "2  Fact: Hydroxychloroquine has been shown to hav...   \n",
       "3  The Corona virus is a man made virus created i...   \n",
       "4  Doesn’t @BillGates finance research at the Wuh...   \n",
       "\n",
       "                      source label  \n",
       "0  coronavirusmedicalkit.com  Fake  \n",
       "1               RudyGiuliani  Fake  \n",
       "2                CharlieKirk  Fake  \n",
       "3    JoanneWrightForCongress  Fake  \n",
       "4    JoanneWrightForCongress  Fake  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fake = df.loc[df['label'] == 'Fake']\n",
    "\n",
    "df_fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Basic protective measures against the new coro...</td>\n",
       "      <td>Stay aware of the latest information on the CO...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Exposing yourself to the sun or to temperature...</td>\n",
       "      <td>You can catch COVID-19, no matter how sunny or...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Being able to hold your breath for 10 seconds ...</td>\n",
       "      <td>The most common symptoms of COVID-19 are dry c...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Drinking alcohol does not protect you against ...</td>\n",
       "      <td>Frequent or excessive alcohol consumption can ...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>COVID-19 virus can be transmitted in areas wit...</td>\n",
       "      <td>From the evidence so far, the COVID-19 virus c...</td>\n",
       "      <td>https://www.who.int/emergencies/diseases/novel...</td>\n",
       "      <td>TRUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "9   Basic protective measures against the new coro...   \n",
       "14  Exposing yourself to the sun or to temperature...   \n",
       "16  Being able to hold your breath for 10 seconds ...   \n",
       "17  Drinking alcohol does not protect you against ...   \n",
       "18  COVID-19 virus can be transmitted in areas wit...   \n",
       "\n",
       "                                                 text  \\\n",
       "9   Stay aware of the latest information on the CO...   \n",
       "14  You can catch COVID-19, no matter how sunny or...   \n",
       "16  The most common symptoms of COVID-19 are dry c...   \n",
       "17  Frequent or excessive alcohol consumption can ...   \n",
       "18  From the evidence so far, the COVID-19 virus c...   \n",
       "\n",
       "                                               source label  \n",
       "9   https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "14  https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "16  https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "17  https://www.who.int/emergencies/diseases/novel...  TRUE  \n",
       "18  https://www.who.int/emergencies/diseases/novel...  TRUE  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true = df.loc[df['label'] == 'TRUE']\n",
    "\n",
    "df_true.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Encontrar colocaciones (2 puntos)\n",
    "\n",
    "Recordad que las colocaciones son términos multipalabra, es decir, secuencias de palabras que tienen un significado en conjunto significativamente diferente del significado derivado de los significados de las palabras individuales  (e.g. New York tiene un significado distinto del que se puede derivar de New y de York)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong>  Computa los mejores bigramas y trigramas de los titulares de las noticias falsas que no están en los titulares de las noticias verdaderas (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importar la lista de stopwords en inglés de la libreria NLTK.\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#Añadir stopwords\n",
    "stopwords = stopwords + ['unknown', 've', 'hadn', 'll', 'didn', 'isn', 'doesn', 'hasn' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este apartado hay que cargar las siguientes librerías:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.collocations import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del comando help(nltk.collocations.BigramAssocMeasures) explora la clase BigramAssocMeasures del módulo nltk.metrics.association y revisa las definiciones de las métricas de Likelihood Ratio (likelihood_ratio) y de Pointwise Mutual Information (pmi) en las secciones que se indican del capítulo 5 del libro Foundations of Statistical Natural Language Processing (Manning & Schutze)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BigramAssocMeasures in module nltk.metrics.association:\n",
      "\n",
      "class BigramAssocMeasures(NgramAssocMeasures)\n",
      " |  A collection of bigram association measures. Each association measure\n",
      " |  is provided as a function with three arguments::\n",
      " |  \n",
      " |      bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)\n",
      " |  \n",
      " |  The arguments constitute the marginals of a contingency table, counting\n",
      " |  the occurrences of particular events in a corpus. The letter i in the\n",
      " |  suffix refers to the appearance of the word in question, while x indicates\n",
      " |  the appearance of any word. Thus, for example:\n",
      " |  \n",
      " |  - n_ii counts ``(w1, w2)``, i.e. the bigram being scored\n",
      " |  - n_ix counts ``(w1, *)``\n",
      " |  - n_xi counts ``(*, w2)``\n",
      " |  - n_xx counts ``(*, *)``, i.e. any bigram\n",
      " |  \n",
      " |  This may be shown with respect to a contingency table::\n",
      " |  \n",
      " |              w1    ~w1\n",
      " |           ------ ------\n",
      " |       w2 | n_ii | n_oi | = n_xi\n",
      " |           ------ ------\n",
      " |      ~w2 | n_io | n_oo |\n",
      " |           ------ ------\n",
      " |           = n_ix        TOTAL = n_xx\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BigramAssocMeasures\n",
      " |      NgramAssocMeasures\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  chi_sq(n_ii, n_ix_xi_tuple, n_xx) from abc.ABCMeta\n",
      " |      Scores bigrams using chi-square, i.e. phi-sq multiplied by the number\n",
      " |      of bigrams, as in Manning and Schutze 5.3.3.\n",
      " |  \n",
      " |  fisher(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using Fisher's Exact Test (Pedersen 1996).  Less\n",
      " |      sensitive to small counts than PMI or Chi Sq, but also more expensive\n",
      " |      to compute. Requires scipy.\n",
      " |  \n",
      " |  phi_sq(*marginals) from abc.ABCMeta\n",
      " |      Scores bigrams using phi-square, the square of the Pearson correlation\n",
      " |      coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  dice(n_ii, n_ix_xi_tuple, n_xx)\n",
      " |      Scores bigrams using Dice's coefficient.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  jaccard(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Jaccard index.\n",
      " |  \n",
      " |  likelihood_ratio(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using likelihood ratios as in Manning and Schutze 5.3.4.\n",
      " |  \n",
      " |  pmi(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams by pointwise mutual information, as in Manning and\n",
      " |      Schutze 5.4.\n",
      " |  \n",
      " |  poisson_stirling(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using the Poisson-Stirling measure.\n",
      " |  \n",
      " |  student_t(*marginals) from abc.ABCMeta\n",
      " |      Scores ngrams using Student's t test with independence hypothesis\n",
      " |      for unigrams, as in Manning and Schutze 5.3.1.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  mi_like(*marginals, **kwargs)\n",
      " |      Scores ngrams using a variant of mutual information. The keyword\n",
      " |      argument power sets an exponent (default 3) for the numerator. No\n",
      " |      logarithm of the result is calculated.\n",
      " |  \n",
      " |  raw_freq(*marginals)\n",
      " |      Scores ngrams by their frequency\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from NgramAssocMeasures:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.collocations.BigramAssocMeasures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer paso: Computa los tokens de los titulares de las noticias falsas. Etiqueta estos tokens por su PoS. Ten en cuenta que hay noticias sin titular y que puede haber titulares con palabras que tengan caracteres especiales al principio. Si una noticia no tiene titular (NaN en la columna 'title') sustituimos NaN por 'empty'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sustituimos 'Nan' por 'empty'\n",
    "df_no_na = df.fillna('empty')\n",
    "\n",
    "#Creamos un dataframe que contiene las noticias falsas\n",
    "df_fake = df_no_na.loc[df['label'] == 'Fake']\n",
    "\n",
    "#Creamos un dataframe que contiene las noticias verdaderas\n",
    "df_true = df_no_na.loc[df['label'] == 'TRUE']\n",
    "\n",
    "#Quitamos los titulares vacíos \n",
    "\n",
    "titulares_fake_noempty = [fh for fh in df_fake['title'].to_list() if fh != 'empty']\n",
    "titulares_true_noempty = [th for th in df_true['title'].to_list() if th != 'empty']\n",
    "\n",
    "#Creamos un texto en minúscula con todos los titulares falsos\n",
    "\n",
    "titulares_fake = \" \".join(titulares_fake_noempty).lower()\n",
    "\n",
    "#Creamos un texto en minúscula con todos los titulares verdaderos\n",
    "\n",
    "titulares_true = \" \".join(titulares_true_noempty).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                   #\n",
    "#############################################\n",
    "from nltk import word_tokenize\n",
    "alpha_tokens = [w for w in word_tokenize(titulares_fake) if re.match(\"^[a-z]+.*\", w)]\n",
    "tagged_tokens = nltk.pos_tag(alpha_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo paso: Computa los 1000 mejores bigramas y los 1000 mejores trigramas a partir de los tokens etiquetados (e.g. [(Basic, JJ), ...]) de los titulares falsos. Utiliza las métricas PMI y la Likehood Ratio. Tienes que comentar las similitudes y diferencias que encuentras en los resultados según la métrica utilizada.\n",
    "\n",
    "<b>Atención</b>: De los 1000 bigramas y trigramas, elige los que no empiezan ni terminan con una stopword."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos la clasificación de etiquetas PoS.\n",
    "\n",
    "<b>Etiquetas PoS</b>\n",
    "\n",
    "<ul>\n",
    "<li>DT: Determinante</li>\n",
    "<li>JJ: Adjetivo</li>\n",
    "<li>NN: Nombre en singular</li>\n",
    "<li>NNS: Nombre en plural</li>\n",
    "<li>VBD: Verbo en pasado</li>\n",
    "<li>VBG: Verbo en gerundio</li>\n",
    "<li>MD: Verbo modal</li>\n",
    "<li>IN: Preposición o conjunción subordinada</li>\n",
    "<li>PRP: Pronombre</li>\n",
    "<li>RB: Adverbio</li>\n",
    "<li>RP: Partícula</li>    \n",
    "<li>CC: Conjunción coordinada</li>\n",
    "<li>CD: Numeral</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "def get_coll_candidates(tokens):\n",
    "    bigramcandidates = BigramCollocationFinder.from_words(tokens)\n",
    "    trigramcandidates = TrigramCollocationFinder.from_words(tokens)\n",
    "    return bigramcandidates , trigramcandidates\n",
    "\n",
    "def get_n_best_candidates(bigram_candidates, trigram_candidates, n_best_collocations, metrica='pmi'):\n",
    "    if metrica == 'pmi':\n",
    "        nbest_bigram_candidates = bigram_candidates.nbest(bigram_measures.pmi,n_best_collocations)\n",
    "        nbest_trigram_candidates = trigram_candidates.nbest(trigram_measures.pmi,n_best_collocations)\n",
    "    \n",
    "    else:\n",
    "        nbest_bigram_candidates = bigram_candidates.nbest(bigram_measures.likelihood_ratio,n_best_collocations)\n",
    "        nbest_trigram_candidates = trigram_candidates.nbest(trigram_measures.likelihood_ratio,n_best_collocations)\n",
    "      \n",
    "    return nbest_bigram_candidates, nbest_trigram_candidates\n",
    "\n",
    "def good_stw_candidate(candidate):\n",
    "    test = True\n",
    "    if type(candidate) == str:\n",
    "        if candidate in stopwords:\n",
    "            test = False\n",
    "    else:\n",
    "        if len(candidate)==1:\n",
    "            if candidate[0] in stopwords:\n",
    "                test = False\n",
    "        else:\n",
    "            if len(candidate)==2:\n",
    "                if candidate[0][0] in stopwords or candidate[-1][0] in stopwords:\n",
    "                    test = False\n",
    "            else:\n",
    "                if candidate[0][0] in stopwords or candidate[1][0] in stopwords or candidate[-1][0] in stopwords:\n",
    "                    test = False\n",
    "    return test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas con métrica PMI  :  761\n",
      "Bigramas con métrica LR   :  690\n",
      "Trigramas con métrica PMI :  527\n",
      "Trigramas con métrica LR  :  430\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "bigram_coll_candidates, trigram_coll_candidates = get_coll_candidates(tagged_tokens)\n",
    "\n",
    "nbest_bigram_candidates, nbest_trigram_candidates = get_n_best_candidates(bigram_coll_candidates, \n",
    "                                                                          trigram_coll_candidates, \n",
    "                                                                          1000)\n",
    "nbest_bigram_candidates_lr, nbest_trigram_candidates_lr = get_n_best_candidates(bigram_coll_candidates, \n",
    "                                                                          trigram_coll_candidates, \n",
    "                                                                          1000,\n",
    "                                                                          'lr')\n",
    "nbc_pmi=[] #bigramas con metrica pmi sin stopword\n",
    "nbc_lr=[]  #bigramas con metrica lr sin stopword\n",
    "ntc_pmi=[] #trigramas con metrica pmi sin stopword\n",
    "ntc_lr=[]  #trigramas con metrica lr sin stopword\n",
    "\n",
    "for i in range(0,1000):\n",
    "    if good_stw_candidate(nbest_bigram_candidates[i]):\n",
    "        nbc_pmi.append(nbest_bigram_candidates[i])\n",
    "    if good_stw_candidate(nbest_bigram_candidates_lr[i]):\n",
    "        nbc_lr.append(nbest_bigram_candidates_lr[i]) \n",
    "    if good_stw_candidate(nbest_trigram_candidates[i]):\n",
    "        ntc_pmi.append(nbest_trigram_candidates[i])\n",
    "    if good_stw_candidate(nbest_trigram_candidates_lr[i]):\n",
    "        ntc_lr.append(nbest_trigram_candidates_lr[i])  \n",
    "        \n",
    "        \n",
    "        \n",
    "print('Bigramas con métrica PMI  : ',len(nbc_pmi))\n",
    "print('Bigramas con métrica LR   : ',len(nbc_lr))\n",
    "print('Trigramas con métrica PMI : ',len(ntc_pmi))\n",
    "print('Trigramas con métrica LR  : ',len(ntc_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong>  Escribe un comentario sobre los resultados obtenidos, contestando las siguientes preguntas: ¿Con las métricas utilizadas, es posible encontrar características distintivas en los ngramas de los titulares de noticias falsas? ¿Qué métrica te parece más adecuada para realizar este análisis? ¿Sería más eficiente detectar solamente ngramas que cumplen el patrón sintáctico de un sintagma nominal (e.g: adjetivo + nombre en singular/plural y nombre + nombre)? (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Vectorizar palabras y términos (3 puntos)\n",
    "\n",
    "Exploraremos la vectorización de palabras y términos con el método Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que el paquete gensim implementa un método para entrenar modelos Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Obtén los términos relacionados con 'coronavirus' en las noticias falsas y los términos relacionados con 'coronavirus' en las noticias verdaderas. Utiliza el cálculo de similitud semántica de un modelo word2vec (2 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer paso: Entrena un modelo de detección de phrases en una oración. Para el entrenamiento utiliza todos los titulares y los cuerpos (no vacíos) de las noticias falsas y verdaderas. Utiliza el módulo Phraser de Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Phrases\n",
    "\n",
    "ss1 = [fh for fh in df_fake['title'].to_list() if fh != 'empty'] \n",
    "ss2 = [th for th in df_true['title'].to_list() if th != 'empty']\n",
    "ss3 = [ft for ft in df_fake['text'].to_list() if ft != 'empty']\n",
    "ss4 = [tt for tt in df_true['text'].to_list() if tt != 'empty']\n",
    "sentence_stream = ss1 + ss2 + ss3 + ss4 \n",
    "sentence_stream = \" \".join(sentence_stream).lower()\n",
    "\n",
    "text_stream = [w for w in word_tokenize(sentence_stream) if re.match(\"^[a-z]+.*\", w)]\n",
    "\n",
    "phrases = Phrases(text_stream, min_count=1, threshold=2)\n",
    "\n",
    "phraser = Phraser(phrases)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo paso: Transforma cada frase de las noticias fake en una lista de phrases lematizadas\n",
    "\n",
    "<b>Atención</b>: Las phrases no deben ser stopwords. Tampoco deben empezar ni terminar con una stopword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m ss\u001b[38;5;241m=\u001b[39mss1\u001b[38;5;241m+\u001b[39mss3 \u001b[38;5;66;03m#titulares y texto de las noticias falsas\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#ss=\" \".join(ss).lower()\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m ss\u001b[38;5;241m=\u001b[39m\u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[1;32m     48\u001b[0m lemmas \u001b[38;5;241m=\u001b[39m  [transform_sentence(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ss]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[38;5;241m6\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "no_pos_in = ['DT', 'IN', 'PRP', 'CC', 'CD','MD', 'VBG', 'VBD', 'RP', 'RB']\n",
    "def get_wn_pos(pos):\n",
    "    if re.match(r'^N',pos):\n",
    "        wn_pos = 'n'\n",
    "    elif re.match(r'^V',pos):\n",
    "        wn_pos = 'v'\n",
    "    else:\n",
    "        wn_pos = 'n' #En inglés, los lemas de términos que no son verbos ni nombres se obtienen como si fueran\n",
    "                        #nombres\n",
    "    return wn_pos\n",
    "\n",
    "#La función wnlemmatize lematiza el término con una etiqueta PoS según el lematizador de Wordnet\n",
    "def wnlemmatize(t,postag):\n",
    "    lemma = \"\"\n",
    "    #Definición del lematizador\n",
    "    lem = WordNetLemmatizer()\n",
    "    #Si el candidato es monopalabra, se obtiene el lema con el lematizador de WordNet según su PoS\n",
    "    if ' ' not in t:\n",
    "        #lemma = lem.lemmatize(t,get_wn_pos(postag[0][1]))\n",
    "        lemma = lem.lemmatize(t,get_wn_pos(postag[1]))\n",
    "    #Si el candidato es multipalabra, obtenemos su lema como si fuera un nombre, aplicando el lematizador de WordNet\n",
    "    else:\n",
    "        lemma = lem.lemmatize(t,'n')\n",
    "    return lemma\n",
    "\n",
    "def transform_sentence(ss):\n",
    "    ss=ss.lower()\n",
    "    text_stream = [w for w in word_tokenize(ss) if re.match(\"^[a-z]+.*\", w) \n",
    "                                            and good_stw_candidate(w)\n",
    "                                            and nltk.pos_tag([w])[0][1] not in no_pos_in \n",
    "              ]\n",
    "    text_phrases=phraser[text_stream]\n",
    "\n",
    "    tagged_phrases = nltk.pos_tag(text_phrases)\n",
    "    lemmas=[]\n",
    "    for tm in range(0,len(text_phrases)):\n",
    "        lemmas.append(wnlemmatize(text_phrases[tm],tagged_phrases[tm]))\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "ss=ss1+ss3 #titulares y texto de las noticias falsas\n",
    "#ss=\" \".join(ss).lower()\n",
    "ss=ss.lower()\n",
    "lemmas =  [transform_sentence(i) for i in ss]\n",
    "print([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "covid and the terror of uncertainty\n"
     ]
    }
   ],
   "source": [
    "print(transform_sentence(ss[4]))\n",
    "print(ss[4].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tercer paso: Crear un modelo word2vec de los titulares y los cuerpos de noticias falsas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due to the recent outbreak for the Coronavirus (COVID-19) the World Health Organization is giving away vaccine kits. Just pay $4.95 for shipping\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "def transform_sentence(sentence):\n",
    "    #Obtenemos los phrases según el modelo de detección de phrases que ha aprendido\n",
    "    sentence_phrases = phrases[word_tokenize(sentence.lower())]\n",
    "    #Quitamos los signos de puntuación de los phrases\n",
    "    phrases_stripped = [st.strip('\".,;:-():!?-‘’ ') for st in sentence_phrases if re.match(\"^[a-z]+.*\", st)]\n",
    "    #Hacemos etiquetaje de PoS de los phrases y lo guardamos en un diccionario (postag) \n",
    "    for ps in phrases_stripped:\n",
    "        postag[ps] = nltk.pos_tag(word_tokenize(ps))\n",
    "    #Lematizamos los phrases con el lematizador de Wordnet\n",
    "    phrases_lemmatized = [wnlemmatize(ps,postag[ps]) for ps in phrases_stripped]\n",
    "    #Unificamos los phrases\n",
    "    phrases_lemmatized_and_unified = [unify(pl) for pl in phrases_lemmatized ]\n",
    "    return phrases_lemmatized_and_unified\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sentence_stream=ss1+ss3\n",
    "postag={}\n",
    "print(sentence_stream[0])\n",
    "#transformed_sentences = [transform_sentence(ss) for ss in sentence_stream ]\n",
    "#print(transformed_sentences[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuarto paso: Seleccionar el vocabulario del modelo word2vec sobre el cual se verán los términos parecidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quinto paso: Presentar los términos fake más cercanos semánticamente a 'coronavirus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sexto paso: Realiza los pasos anteriores pero con las frases de las noticias verdaderas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Comenta las diferencias que existen en los términos relacionados con 'coronavirus' en las noticias falsas y en las noticias verdaderas. ¿Crees que estas diferencias son representativas de los contenidos de las noticias falsas? (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detección de temas. (4 puntos)\n",
    "\n",
    "En estos apartados exploraremos los temas tratados en las noticias falsas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Exploración de los temas con WordNet (2 puntos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado accederemos a Wordnet a través de la librería nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Comprueba si las noticias falsas contienen términos alejados semánticamente del sentido del término 'disease' en Wordnet. Compruébalo calculando la similitud de Wu and Palmer entre el sentido de wordnet 'disease.n.01' y los términos relacionados con 'coronavirus' en el modelo word2vec de las noticias falsas. (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer paso: Calcula la distancia Wu and Palmer entre el sentido 'disease.n.01' y el primer sentido de los sustantivos más relacionados con 'coronavirus' en el modelo word2vec de las noticias falsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Comenta los resultados que te han llamado más la atención. Fíjate en los términos que no están en Wordnet y los que tienen una distancia muy alejada al sentido de disease.n.01. ¿Crees que Wordnet es un buen recurso para analizar los temas que se tratan en las noticias falsas sobre el coronavirus? (1 punto)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LDA (2 puntos)\n",
    "\n",
    "Recordemos que en el notebook del módulo 1 hemos visto la aplicación del método LDA para extraer temas de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Extrae temas a partir de los phrases de los titulares y los cuerpos de noticias falsas. Lo haremos con el método LDA. Experimenta con el parámetro num_topics hasta encontrar un conjunto de temas informativo, y asigna nombres a los temas encontrados. La construcción del modelo LDA puede tardar más de 10 minutos en algunos casos (2 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clasificación (1 punto)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Crea un clasificador automático de noticias falsas y no falsas a partir de los titulares. (0.5 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer paso: Unimos el dataframe con las noticias falsas y el dataframe con las noticias verdaderas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo paso: Realizamos dos listas. Una con los titulares y otra con las etiquetas correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tercer paso: Vectorizamos los titulares con un vectorizador tf.idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuarto paso:Preparamos el corpus de entrenamiento y de evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quinto paso: Entrenar el clasificador con Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sexto paso: Utilizar el modelo entrenado para predecir la categoría Fake o True de los titulares del conjunto de test y mostrar las palabras más informativas para cada categoría. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# SOLUCIÓN                                  #\n",
    "#############################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> A partir de las palabras más informativas, ¿qué contenidos crees que son típicos de las noticias falsas sobre el coronavirus? ¿Crees que hay que considerar elementos formales (e.g: ausencia de titular, uso de mayúsculas) como distintivos de las noticias falsas? (0.5 puntos)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
